{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pickle\n","from typing import Optional, Tuple\n","\n","class ClusteredGateNetwork(nn.Module):\n","    def __init__(self, num_clusters: int, num_heads: int, seq_len: int):\n","        super().__init__()\n","        self.num_clusters = num_clusters\n","        self.gates = nn.ModuleList([\n","            AdaptiveGateNetwork(num_heads, seq_len) for _ in range(num_clusters)\n","        ])\n","\n","    def forward(self, attn_weights: torch.Tensor, cluster_ids: torch.Tensor):\n","        \"\"\"\n","        attn_weights: (B, num_heads, N, N)\n","        cluster_ids: (B,) long tensor of cluster ids (0, 1, 2)\n","        Returns:\n","            gated_attn: (B, num_heads, N, N)\n","        \"\"\"\n","        B, H, N, _ = attn_weights.shape\n","        gated = torch.zeros_like(attn_weights)\n","        for b in range(B):\n","            cid = cluster_ids[b].item()\n","            if cid == -1:\n","                gated[b] = attn_weights[b]\n","                continue\n","            gate = self.gates[cid]\n","            for h in range(H):\n","                gated_attn = gate(attn_weights[b:b+1, h:h+1])\n","                gated[b, h] = gated_attn.squeeze(0)\n","        return gated\n"],"metadata":{"id":"e9QRg5SgBjFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4f1gFbL1PCp"},"outputs":[],"source":["class AdaptiveGateNetwork(nn.Module):\n","    def __init__(self, num_heads: int, seq_len: int):\n","        super().__init__()\n","        self.gate = nn.Sequential(\n","            nn.LayerNorm([seq_len, seq_len]),\n","            nn.Linear(seq_len, seq_len),\n","            nn.ReLU(),\n","            nn.Linear(seq_len, seq_len)\n","        )\n","\n","    def forward(self, attn_weights: torch.Tensor):\n","        B, H, N, _ = attn_weights.shape\n","        x = attn_weights.squeeze(1)  # (B, N, N)\n","        x = self.gate(x)\n","        return x.unsqueeze(1)  # (B, 1, N, N)\n","\n","\n","class GatedAttentionBlock(nn.Module):\n","    def __init__(self, original_attn: nn.Module, gate_network: ClusteredGateNetwork):\n","        super().__init__()\n","        self.original_attn = original_attn\n","        self.gate_network = gate_network\n","        self.qkv = original_attn.qkv\n","        self.scale = original_attn.scale\n","        self.proj = original_attn.proj\n","        self.num_heads = original_attn.num_heads\n","        self.head_dim = original_attn.head_dim\n","\n","        self.apply_gating = False\n","        self.cluster_ids = None\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = F.softmax(attn, dim=-1)\n","\n","        if self.apply_gating and self.cluster_ids is not None:\n","            attn = self.gate_network(attn, self.cluster_ids)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        return self.proj(out)\n","\n","\n"]},{"cell_type":"code","source":["class Detector(nn.Module):\n","    def __init__(self,vit, num_blocks=3):\n","        super().__init__()\n","        self.vit = vit\n","        self.fc = nn.Sequential(\n","            nn.Linear(vit.num_features, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","        self.num_blocks = num_blocks\n","    def forward(self, x):\n","        x = self.vit.patch_embed(x)\n","        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)\n","        x = torch.cat((cls_token, x), dim=1)\n","        x = x + self.vit.pos_embed\n","        x = self.vit.pos_drop(x)\n","\n","        for i in range(self.num_blocks):\n","            x = self.vit.blocks[i](x)\n","\n","        x = self.vit.norm(x)\n","        feats = x[:, 0]\n","\n","        out = self.fc(feats)\n","        return torch.sigmoid(out).squeeze()\n","\n"],"metadata":{"id":"2r6DXC3Q54iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pickle\n","from typing import Optional, Tuple\n","\n","class ViTDefenseSystem(nn.Module):\n","    def __init__(self,\n","                 base_model,\n","                 classifier_path: str,\n","                 diagnoser_path: str,\n","                 diagnoser_head_path: str,\n","                 num_clusters: int,\n","                 detection_threshold: float = 0.5):\n","        super().__init__()\n","\n","        self.base_model = base_model\n","        self.detection_threshold = detection_threshold\n","        self.num_clusters = num_clusters\n","\n","        print(\"Freezing base ViT model...\")\n","        for name, param in self.base_model.named_parameters():\n","            param.requires_grad = False\n","        self.base_model.eval()\n","        print(f\"Frozen {sum(p.numel() for p in self.base_model.parameters())} base model parameters\")\n","\n","\n","        vit_dummy = timm.create_model('vit_base_patch16_224', pretrained=True)\n","        detector = Detector(vit_dummy, num_blocks=6)\n","\n","        detector.load_state_dict(torch.load(classifier_path, map_location='cpu'))\n","\n","        self.classifier = detector.fc\n","\n","        self.classifier.eval()\n","        for p in self.classifier.parameters():\n","            p.requires_grad = False\n","\n","        with open(diagnoser_path, 'rb') as f:\n","            self.diagnoser_utils = pickle.load(f)\n","\n","        self.diagnoser_head = AttentionDiagnosisHead(input_dim=768, hidden_dim=64)\n","        self.diagnoser_head.load_state_dict(torch.load(diagnoser_head_path, map_location='cpu'))\n","        self.diagnoser_head.eval()\n","\n","\n","\n","        for p in self.diagnoser_head.parameters():\n","            p.requires_grad = False\n","\n","        dummy_input = torch.randn(1, 3, 224, 224)\n","        with torch.no_grad():\n","            _ = base_model.patch_embed(dummy_input)\n","            seq_len = base_model.pos_embed.shape[1]\n","            num_heads = base_model.blocks[0].attn.num_heads\n","\n","        self.gated_blocks = nn.ModuleList()\n","        self.gate_network = ClusteredGateNetwork(num_clusters, num_heads, seq_len)\n","\n","        for i, block in enumerate(self.base_model.blocks):\n","            if i >= 6:\n","                gated_attn = GatedAttentionBlock(block.attn, self.gate_network)\n","                block.attn = gated_attn\n","            self.gated_blocks.append(block)\n","        self._verify_freezing()\n","\n","    def _verify_freezing(self):\n","\n","        total_params = 0\n","        trainable_params = 0\n","        frozen_params = 0\n","\n","        for name, param in self.named_parameters():\n","            total_params += param.numel()\n","            if param.requires_grad:\n","                trainable_params += param.numel()\n","                print(f\"TRAINABLE: {name} ({param.numel()} params)\")\n","            else:\n","                frozen_params += param.numel()\n","\n","        print(f\"\\nSUMMARY:\")\n","        print(f\"Total parameters: {total_params:,}\")\n","        print(f\"Trainable parameters: {trainable_params:,}\")\n","        print(f\"Frozen parameters: {frozen_params:,}\")\n","        print(f\"Trainable percentage: {100*trainable_params/total_params:.2f}%\")\n","\n","        expected_trainable = sum(p.numel() for p in self.gate_network.parameters())\n","        if trainable_params == expected_trainable:\n","            print(\"SUCCESS: Only gate network parameters are trainable!\")\n","        else:\n","            print(\"ERROR: Unexpected trainable parameters detected!\")\n","            print(f\"Expected {expected_trainable}, got {trainable_params}\")\n","\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n","        B = x.size(0)\n","        debug_info = {}\n","\n","        x = self.base_model.patch_embed(x)\n","        cls_token = self.base_model.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_token, x), dim=1)\n","        x = self.base_model.pos_drop(x + self.base_model.pos_embed)\n","\n","        cluster_ids = torch.full((B,), -1, dtype=torch.long, device=x.device)\n","\n","        for i, block in enumerate(self.base_model.blocks):\n","            if i == 5:\n","                cls_embeddings = x[:, 0]\n","                with torch.no_grad():\n","                    adv_probs = self.classifier(cls_embeddings)\n","                    adv_probs = torch.sigmoid(adv_probs).squeeze()\n","                    is_adv = adv_probs > self.detection_threshold\n","\n","                cluster_ids = self._diagnose_cluster(cls_embeddings, is_adv)\n","                debug_info[\"cluster_ids\"] = cluster_ids\n","                debug_info[\"is_adversarial\"] = is_adv\n","\n","                for j in range(i + 1, len(self.base_model.blocks)):\n","                    attn = self.base_model.blocks[j].attn\n","                    if isinstance(attn, GatedAttentionBlock):\n","                        attn.apply_gating = True\n","                        attn.cluster_ids = cluster_ids\n","\n","            x = block(x)\n","\n","        x = self.base_model.norm(x)\n","        logits = self.base_model.head(x[:, 0])\n","\n","        return logits, debug_info\n","\n","    def _diagnose_cluster(self, cls_embeddings: torch.Tensor, is_adv: torch.Tensor) -> torch.Tensor:\n","\n","        emb_np = cls_embeddings.detach().cpu().numpy()\n","\n","        if emb_np.shape[1] > self.diagnoser_utils['pca'].n_features_in_:\n","            emb_np = emb_np[:, :self.diagnoser_utils['pca'].n_features_in_]\n","\n","        pca_features = self.diagnoser_utils['pca'].transform(emb_np)\n","        cluster_preds = self.diagnoser_utils['kmeans'].predict(pca_features)\n","        return torch.tensor(cluster_preds, device=cls_embeddings.device)"],"metadata":{"id":"uSQsLcLP2tvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import pandas as pd\n","import pickle\n","import re\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from torchvision import transforms\n","import timm\n","from timm.data import resolve_data_config\n","from timm.data.transforms_factory import create_transform\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","vit = timm.create_model('vit_base_patch16_224', pretrained=True).eval()\n","config = resolve_data_config({}, model=vit)\n","transform = create_transform(**config)\n","\n","def extract_original_filename(path):\n","    basename = os.path.basename(path)\n","    match = re.search(r'ILSVRC2012_val_\\d+\\.JPEG', basename)\n","    if match:\n","        return match.group(0)\n","    return basename\n","\n","def save_used_basenames(train_csv, val_csv, output_path):\n","    used = set()\n","\n","    train_df = pd.read_csv(train_csv)\n","    train_used = set(train_df['image_path'].apply(extract_original_filename))\n","    used.update(train_used)\n","    print(f\"Found {len(train_used)} unique images in train adversarial dataset\")\n","\n","    val_df = pd.read_csv(val_csv)\n","    val_used = set(val_df['image_path'].apply(extract_original_filename))\n","    used.update(val_used)\n","    print(f\"Found {len(val_used)} unique images in val adversarial dataset\")\n","\n","    print(f\"Total unique images across both datasets: {len(used)}\")\n","    print(f\"Overlap between train and val adversarial: {len(train_used.intersection(val_used))}\")\n","\n","    with open(output_path, 'wb') as f:\n","        pickle.dump(used, f)\n","    print(f\"Saved {len(used)} original ILSVRC basenames to {output_path}\")\n","    print(\"These images will be excluded when adding extra clean samples to training\")\n","\n","def load_used_basenames(pkl_path):\n","    with open(pkl_path, 'rb') as f:\n","        used = pickle.load(f)\n","    return used\n","\n","used_filenames = load_used_basenames('/content/drive/MyDrive/my231n/used_basenames.pkl')\n","\n","def get_extra_clean_examples(val_dir, used_filenames, n=7000):\n","    \"\"\"Get extra clean examples from val directory, excluding already used images\"\"\"\n","    all_clean = []\n","    for root, _, files in os.walk(val_dir):\n","        for file in files:\n","            if file.endswith('.JPEG') and file not in used_filenames:\n","                all_clean.append(os.path.relpath(os.path.join(root, file), val_dir))\n","\n","    if len(all_clean) < n:\n","        print(f\"Warning: Requested {n} clean samples, but only found {len(all_clean)} unused ones.\")\n","        print(f\"Using all {len(all_clean)} available unused samples.\")\n","        n = len(all_clean)\n","\n","    sampled = random.sample(all_clean, n)\n","    new_rows = [{\n","        'image_path': os.path.join('val', path),\n","        'attack_type': 'clean',\n","        'original_class': -1\n","    } for path in sampled]\n","    return pd.DataFrame(new_rows)\n","\n","class AdversarialDetectionDataset(Dataset):\n","    def __init__(self, metadata_csv, root_dir, split, transform):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        if split == 'train':\n","            self.df = pd.read_csv(metadata_csv)\n","            self.df = self.df[self.df['attack_type'] != 'CW']\n","\n","            val_df = pd.read_csv('/content/drive/MyDrive/my231n/adversarial_val_dataset/metadata_with_clean.csv')\n","            val_original_names = set(val_df['image_path'].apply(extract_original_filename))\n","\n","            original_len = len(self.df)\n","            train_original_names = self.df['image_path'].apply(extract_original_filename)\n","            overlap_mask = train_original_names.isin(val_original_names)\n","            self.df = self.df[~overlap_mask]\n","\n","            print(f\"Removed {overlap_mask.sum()} overlapping images from training set\")\n","            print(f\"Training set reduced from {original_len} to {len(self.df)} samples\")\n","\n","            clean_df = self.df[self.df['attack_type'].str.lower() == 'clean']\n","            adv_df = self.df[self.df['attack_type'].str.lower() != 'clean']\n","\n","            extra_clean_df = get_extra_clean_examples(\n","                val_dir=os.path.join(root_dir, 'val'),\n","                used_filenames=used_filenames,\n","                n=8512\n","            )\n","            print(f\"Dataset composition: {len(clean_df)} original clean, {len(adv_df)} adversarial, {len(extra_clean_df)} extra clean\")\n","            self.df = pd.concat([clean_df, adv_df, extra_clean_df], ignore_index=True)\n","\n","        elif split == 'val':\n","            self.df = pd.read_csv(metadata_csv)\n","            split_keyword = 'adversarial_val_dataset'\n","            self.df = self.df[self.df['image_path'].str.contains(split_keyword)]\n","        else:\n","            raise ValueError(f\"Unsupported split: {split}\")\n","\n","        self.df = self.df.reset_index(drop=True)\n","        if len(self.df) == 0:\n","            raise ValueError(f\"No data found for split '{split}'.\")\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        rel_path = row['image_path']\n","        full_path = rel_path if os.path.isabs(rel_path) else os.path.join(self.root_dir, rel_path)\n","        label = 1 if row['attack_type'].lower() == 'clean' else 0\n","\n","        try:\n","            image = Image.open(full_path).convert('RGB')\n","            if self.transform:\n","                image = self.transform(image)\n","        except Exception as e:\n","            print(f\"Failed to load {full_path}: {e}\")\n","            image = torch.zeros((3, 224, 224))\n","            label = -1\n","\n","        return image, label\n","\n","metadata_csv_path_train = '/content/drive/MyDrive/my231n/adversarial_train_dataset/metadata_with_clean.csv'\n","metadata_csv_path_val = '/content/drive/MyDrive/my231n/adversarial_val_dataset/metadata_with_clean.csv'\n","image_root = '/content/drive/MyDrive/my231n/'\n","train_dataset = AdversarialDetectionDataset(metadata_csv_path_train, image_root, split='train', transform=transform)\n","val_dataset = AdversarialDetectionDataset(metadata_csv_path_val, image_root, split='val', transform=transform)\n","\n","from collections import Counter\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n","\n","print(f\"Loaded {len(train_dataset)} training and {len(val_dataset)} validation samples.\")\n","\n","def verify_no_overlap():\n","    \"\"\"Verify there's no overlap between training and validation sets\"\"\"\n","    train_original_names = set()\n","    val_original_names = set()\n","\n","    for _, row in train_dataset.df.iterrows():\n","        original_name = extract_original_filename(row['image_path'])\n","        train_original_names.add(original_name)\n","\n","    for _, row in val_dataset.df.iterrows():\n","        original_name = extract_original_filename(row['image_path'])\n","        val_original_names.add(original_name)\n","\n","    overlap = train_original_names.intersection(val_original_names)\n","    print(f\"Overlap verification: {len(overlap)} overlapping images found\")\n","    if len(overlap) > 0:\n","        print(f\"Warning: Found overlapping images: {list(overlap)[:5]}...\")\n","    else:\n","        print(\"✓ No overlap detected between train and validation sets\")\n","\n","verify_no_overlap()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRdRHoTr3cLs","executionInfo":{"status":"ok","timestamp":1749094810099,"user_tz":420,"elapsed":5755,"user":{"displayName":"atall nothing","userId":"01016788521006154765"}},"outputId":"c016f516-3fd3-46ee-c099-20fafe90fb2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Removed 2000 overlapping images from training set\n","Training set reduced from 9728 to 7728 samples\n","Warning: Requested 8512 clean samples, but only found 0 unused ones.\n","Using all 0 available unused samples.\n","Dataset composition: 966 original clean, 6762 adversarial, 0 extra clean\n","Loaded 7728 training and 13068 validation samples.\n","Overlap verification: 0 overlapping images found\n","✓ No overlap detected between train and validation sets\n"]}]},{"cell_type":"code","source":["class AttentionDiagnosisHead(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64):\n","        super().__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 2)\n","        )\n"],"metadata":{"id":"FScd_-IaAtGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6pe8oiX_7Ky","executionInfo":{"status":"ok","timestamp":1749094810657,"user_tz":420,"elapsed":548,"user":{"displayName":"atall nothing","userId":"01016788521006154765"}},"outputId":"4a9b46e6-76a6-4970-f4d1-3e3dbe153836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import timm\n","\n","vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n","vit.head = nn.Linear(vit.head.in_features, 1000)\n","classifier_path = '/content/drive/MyDrive/my231n/cs231n_project/detector_epoch_final.pth'\n","diagnoser_path = '/content/drive/MyDrive/my231n/cs231n_project/diagnoser_utils.pkl'\n","diagnoser_head_path = '/content/drive/MyDrive/my231n/cs231n_project/diagnoser_epoch_5.pt'\n","\n","defense_model = ViTDefenseSystem(\n","    base_model=vit,\n","    classifier_path=classifier_path,\n","    diagnoser_path=diagnoser_path,\n","    diagnoser_head_path=diagnoser_head_path,\n","    num_clusters=3,\n","    detection_threshold=0.5\n",")\n"],"metadata":{"id":"6kgwxqZZ36Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","optimizer = torch.optim.Adam(defense_model.gate_network.parameters(), lr=1e-4)\n","save_every = 10\n","checkpoint_dir = \"checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","step = 0\n","for images, labels in tqdm(train_loader):\n","    logits, debug = defense_model(images)\n","    loss = F.cross_entropy(logits, labels)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    if step % save_every == 0:\n","        ckpt_path = os.path.join(checkpoint_dir, f\"defense_step_{step}.pt\")\n","        torch.save({\n","            'step': step,\n","            'model_state_dict': defense_model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss.item(),\n","        }, ckpt_path)\n","        print(f\"[Checkpoint saved] → {ckpt_path}\")\n","\n","    step += 1\n"],"metadata":{"id":"y_lpI2Y03BmM"},"execution_count":null,"outputs":[]}]}