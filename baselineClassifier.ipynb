{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2198,"status":"ok","timestamp":1748833417425,"user":{"displayName":"Yanny Gao","userId":"02332755913733850809"},"user_tz":420},"id":"hS4KPjvCehZq","outputId":"145afe9f-a7b9-496d-9f8b-f212176a66dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["ViTClassifier(\n","  (model): VisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      (norm): Identity()\n","    )\n","    (pos_drop): Dropout(p=0.0, inplace=False)\n","    (patch_drop): Identity()\n","    (norm_pre): Identity()\n","    (blocks): Sequential(\n","      (0): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (4): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (5): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (6): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (7): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (8): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (9): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (10): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","      (11): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","          (q_norm): Identity()\n","          (k_norm): Identity()\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): Identity()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (drop1): Dropout(p=0.0, inplace=False)\n","          (norm): Identity()\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop2): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): Identity()\n","        (drop_path2): Identity()\n","      )\n","    )\n","    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    (fc_norm): Identity()\n","    (head_drop): Dropout(p=0.0, inplace=False)\n","    (head): Linear(in_features=768, out_features=1000, bias=True)\n","  )\n",")\n"]}],"source":["import torch\n","import torch.nn as nn\n","import timm\n","\n","NUM_CLASSES = 1000\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class ViTClassifier(nn.Module):\n","    def __init__(self, num_classes=NUM_CLASSES):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            'vit_base_patch16_224',\n","            pretrained=True,\n","            num_classes=num_classes\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","model = ViTClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n","\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":881,"status":"ok","timestamp":1748833628666,"user":{"displayName":"Yanny Gao","userId":"02332755913733850809"},"user_tz":420},"id":"yoMulDDcmtSy","outputId":"3aef8b42-436c-45b9-a281-bc164ce9414c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 117MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["CNNClassifier(\n","  (model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=1000, bias=True)\n","  )\n",")\n"]}],"source":["import torchvision.models as models\n","\n","class CNNClassifier(nn.Module):\n","    def __init__(self, num_classes=NUM_CLASSES):\n","        super().__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","cnn_model = CNNClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n","\n","print(cnn_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4454,"status":"ok","timestamp":1748833735180,"user":{"displayName":"Yanny Gao","userId":"02332755913733850809"},"user_tz":420},"id":"xnpFGCz3ej5m","outputId":"0ac4c1e2-a463-4f9f-9d2f-4016b9213ec4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch.optim as optim\n","train_loader = None\n","val_loader = None\n","\n","class ViTClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        import timm\n","        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","model = ViTClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n","model = CNNClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n","\n","def train_one_epoch(model, loader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0\n","    for x, y in loader:\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        optimizer.zero_grad()\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader):\n","    model.eval()\n","    correct = total = 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","            out = model(x)\n","            preds = torch.argmax(out, dim=1)\n","            correct += (preds == y).sum().item()\n","            total += y.size(0)\n","    return correct / total if total > 0 else 0.0\n","\n","def run_training(model, train_loader, val_loader, epochs=5):\n","    for epoch in range(epochs):\n","        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n","        val_acc = evaluate(model, val_loader)\n","        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Acc = {val_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19696,"status":"ok","timestamp":1748836830014,"user":{"displayName":"Yanny Gao","userId":"02332755913733850809"},"user_tz":420},"id":"FUGcyDEEy_V0","outputId":"50b601a4-d559-4b91-935f-7e0d25768e39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XOlU6S1VzVdY"},"outputs":[],"source":["import os\n","import shutil\n","import random\n","\n","source_dir = \"/content/drive/MyDrive/cs231n_project/imagenet_val/val\"\n","target_dir = \"/content/drive/MyDrive/cs231n_project/imagenet_val_subset\"\n","images_per_class = 10\n","\n","os.makedirs(target_dir, exist_ok=True)\n","\n","all_classes = sorted([d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))])\n","\n","for cls in all_classes:\n","    cls_src = os.path.join(source_dir, cls)\n","    cls_dst = os.path.join(target_dir, cls)\n","    os.makedirs(cls_dst, exist_ok=True)\n","\n","    images = sorted(os.listdir(cls_src))\n","    chosen_images = random.sample(images, min(len(images), images_per_class))\n","\n","    for img_name in chosen_images:\n","        shutil.copy2(os.path.join(cls_src, img_name), os.path.join(cls_dst, img_name))\n","\n","print(f\"Subset created at {target_dir} with {len(all_classes)} classes and up to {images_per_class} images each.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}